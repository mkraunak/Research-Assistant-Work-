{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Question number 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Lambda, MaxPooling1D, Conv1D, Flatten, LSTM\n",
    "import keras.backend as K\n",
    "from keras.models import Model,Sequential\n",
    "from keras import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(513, 32)\n",
      "(500, 513, 32)\n",
      "200\n",
      "(200, 513, 45)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "with open('hw4_trs.pkl', 'rb') as f:\n",
    "    data_train = pickle.load(f)\n",
    "    \n",
    "    \n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "\n",
    "train_data=[]\n",
    "\n",
    "for i in range(500):\n",
    "    S=librosa.stft(data_train[i], n_fft=1024, hop_length=512)\n",
    "    train_data.append(S)\n",
    "print(S.shape)\n",
    "train_data=np.stack(train_data)\n",
    "print(train_data.shape)\n",
    "\n",
    "test_data=[]\n",
    "with open('hw4_tes.pkl', 'rb') as f1:\n",
    "    data_test = pickle.load(f1)\n",
    "\n",
    "    \n",
    "for i in range(200):\n",
    "    S=librosa.stft(data_test[i], n_fft=1024, hop_length=512)\n",
    "    test_data.append(S)\n",
    "print(len(test_data))\n",
    "\n",
    "test_data=np.stack(test_data)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 32, 513)\n"
     ]
    }
   ],
   "source": [
    "train_data=np.abs(train_data).transpose((0,2,1))\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 45, 513)\n"
     ]
    }
   ],
   "source": [
    "test_data=np.abs(test_data).transpose((0,2,1))\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Siamese Network\n",
    "## Reference was taken from keras Documentation and stack overflow\n",
    "# Define the tensors for the two input speech signals\n",
    "first_In = Input(shape=(None,513))\n",
    "second_In = Input(shape=(None,513))\n",
    "    \n",
    "# Convolutional Neural Network\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=32, kernel_size=2,activation='relu',strides=1, padding='same',input_shape=(None,513),kernel_initializer='he_normal'))\n",
    "model.add(MaxPooling1D(pool_size=2,strides=1, padding='valid'))\n",
    "model.add(Conv1D(filters=32, kernel_size=2,activation='relu',strides=1, padding='same',kernel_initializer='he_normal'))\n",
    "model.add(MaxPooling1D(pool_size=2,strides=1, padding='valid'))\n",
    "model.add(Conv1D(filters=32, kernel_size=3,activation='relu',strides=1, padding='same',kernel_initializer='he_normal'))\n",
    "model.add(MaxPooling1D(pool_size=2,strides=1, padding='valid'))\n",
    "model.add(Conv1D(filters=32, kernel_size=2, activation='relu', kernel_initializer='he_normal' ))\n",
    "model.add(LSTM(412))\n",
    "    \n",
    "# Generate the encodings (feature vectors) for the two speech signals\n",
    "encoded_1 = model(first_In)\n",
    "encoded_2 = model(second_In)\n",
    "    \n",
    " # Add a customized layer to compute the absolute difference between the encodings\n",
    "L1_layer = Lambda(lambda tensors:K.sum(tensors[0]*tensors[1],axis=-1,keepdims=True)) \n",
    "L1_dot = L1_layer([encoded_1, encoded_2])\n",
    "    \n",
    "# Add a dense layer with a sigmoid unit to generate the similarity score\n",
    "prediction = Dense(1,activation='sigmoid')(L1_dot)\n",
    "    \n",
    "# Connect the inputs with the outputs\n",
    "siamese_net = Model(inputs=[first_In,second_In],outputs=prediction)\n",
    "    \n",
    "# compile the model\n",
    "siamese_net.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating batches of samples of train data\n",
    "from itertools import combinations\n",
    "import random\n",
    "import itertools\n",
    "def generator(train_data):\n",
    "    train_data=np.abs(train_data).transpose((0,2,1))\n",
    "    while True:\n",
    "        for i in range(0,510,10):\n",
    "            if i>=len(train_data)-10:\n",
    "                i=0\n",
    "            p=np.array(list(combinations(train_data[i:i+10],2)))\n",
    "            negative_1=list(itertools.product(train_data[i:i+10],train_data[i+10:500]))\n",
    "            negative_1=random.sample(negative_1,45)\n",
    "            negative_1=np.array(negative_1)\n",
    "            data_List=np.concatenate([p, negative_1])\n",
    "            label=[]\n",
    "            for i in range(45): \n",
    "                \n",
    "                label.append(1)\n",
    "            for i in range(45):\n",
    "                label.append(0)\n",
    "        \n",
    "            c = list(zip(data_List, label))\n",
    "            random.shuffle(c)\n",
    "            data_List, label = zip(*c)\n",
    "            data_List=np.array(data_List)\n",
    "            label=np.array(label)\n",
    "            data_List=data_List.transpose((1,0,2,3))\n",
    "\n",
    "            yield [data_List[0],data_List[1]], label[:90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating batches of samples of test data\n",
    "from itertools import combinations\n",
    "import random\n",
    "import itertools\n",
    "def generator1(test_data):\n",
    "    test_data=np.abs(test_data).transpose((0,2,1))\n",
    "    while True:\n",
    "        for i in range(0,210,10):\n",
    "            if i>=len(test_data)-10:\n",
    "                i=0\n",
    "            p1=np.array(list(combinations(test_data[i:i+10],2)))\n",
    "            negative_2=list(itertools.product(test_data[i:i+10],test_data[i+10:200]))\n",
    "            negative_2=random.sample(negative_2,45)\n",
    "            negative_2=np.array(negative_2)\n",
    "            data_List1=np.concatenate([p1, negative_2])\n",
    "            label1=[]\n",
    "            for i in range(45):\n",
    "                label1.append(1)\n",
    "            for i in range(45):\n",
    "                 label1.append(0)\n",
    "        \n",
    "            c1 = list(zip(data_List1, label1))\n",
    "            random.shuffle(c1)\n",
    "            data_List1, label1 = zip(*c1)\n",
    "            data_List1=np.array(data_List1)\n",
    "            label1=np.array(label1)\n",
    "            data_List1=data_List1.transpose((1,0,2,3))\n",
    "\n",
    "            yield [data_List1[0],data_List1[1]], label1[:90]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/46\n",
      "50/50 [==============================] - 52s 1s/step - loss: 0.7378 - acc: 0.5029\n",
      "Epoch 2/46\n",
      "50/50 [==============================] - 38s 766ms/step - loss: 0.6934 - acc: 0.5000\n",
      "Epoch 3/46\n",
      "50/50 [==============================] - 38s 762ms/step - loss: 0.6923 - acc: 0.5167\n",
      "Epoch 4/46\n",
      "50/50 [==============================] - 38s 761ms/step - loss: 0.6959 - acc: 0.4829\n",
      "Epoch 5/46\n",
      "50/50 [==============================] - 39s 776ms/step - loss: 0.6936 - acc: 0.5036\n",
      "Epoch 6/46\n",
      "50/50 [==============================] - 39s 770ms/step - loss: 0.6930 - acc: 0.5118\n",
      "Epoch 7/46\n",
      "50/50 [==============================] - 39s 774ms/step - loss: 0.6927 - acc: 0.5249\n",
      "Epoch 8/46\n",
      "50/50 [==============================] - 39s 780ms/step - loss: 0.6910 - acc: 0.5189\n",
      "Epoch 9/46\n",
      "50/50 [==============================] - 39s 776ms/step - loss: 0.6924 - acc: 0.5478\n",
      "Epoch 10/46\n",
      "50/50 [==============================] - 38s 760ms/step - loss: 0.6933 - acc: 0.4978\n",
      "Epoch 11/46\n",
      "50/50 [==============================] - 39s 773ms/step - loss: 0.6924 - acc: 0.5153\n",
      "Epoch 12/46\n",
      "50/50 [==============================] - 40s 799ms/step - loss: 0.6863 - acc: 0.5429\n",
      "Epoch 13/46\n",
      "50/50 [==============================] - 40s 799ms/step - loss: 0.6872 - acc: 0.5222\n",
      "Epoch 14/46\n",
      "50/50 [==============================] - 40s 795ms/step - loss: 0.6938 - acc: 0.4980\n",
      "Epoch 15/46\n",
      "50/50 [==============================] - 40s 794ms/step - loss: 0.6921 - acc: 0.5227\n",
      "Epoch 16/46\n",
      "50/50 [==============================] - 39s 776ms/step - loss: 0.6924 - acc: 0.5038\n",
      "Epoch 17/46\n",
      "50/50 [==============================] - 46s 925ms/step - loss: 0.6933 - acc: 0.5202\n",
      "Epoch 18/46\n",
      "50/50 [==============================] - 60s 1s/step - loss: 0.6936 - acc: 0.5147\n",
      "Epoch 19/46\n",
      "50/50 [==============================] - 57s 1s/step - loss: 0.6934 - acc: 0.5076\n",
      "Epoch 20/46\n",
      "50/50 [==============================] - 43s 860ms/step - loss: 0.6929 - acc: 0.5087\n",
      "Epoch 21/46\n",
      "50/50 [==============================] - 51s 1s/step - loss: 0.6927 - acc: 0.5149\n",
      "Epoch 22/46\n",
      "50/50 [==============================] - 61s 1s/step - loss: 0.6930 - acc: 0.5058\n",
      "Epoch 23/46\n",
      "50/50 [==============================] - 54s 1s/step - loss: 0.6915 - acc: 0.5227\n",
      "Epoch 24/46\n",
      "50/50 [==============================] - 52s 1s/step - loss: 0.6929 - acc: 0.5047\n",
      "Epoch 25/46\n",
      "50/50 [==============================] - 59s 1s/step - loss: 0.6913 - acc: 0.5278\n",
      "Epoch 26/46\n",
      "50/50 [==============================] - 47s 940ms/step - loss: 0.6901 - acc: 0.5284\n",
      "Epoch 27/46\n",
      "50/50 [==============================] - 38s 757ms/step - loss: 0.6924 - acc: 0.5182\n",
      "Epoch 28/46\n",
      "50/50 [==============================] - 38s 766ms/step - loss: 0.6905 - acc: 0.5324\n",
      "Epoch 29/46\n",
      "50/50 [==============================] - 41s 828ms/step - loss: 0.6866 - acc: 0.5396\n",
      "Epoch 30/46\n",
      "50/50 [==============================] - 38s 756ms/step - loss: 0.6851 - acc: 0.5373\n",
      "Epoch 31/46\n",
      "50/50 [==============================] - 39s 777ms/step - loss: 0.6845 - acc: 0.5344\n",
      "Epoch 32/46\n",
      "50/50 [==============================] - 38s 762ms/step - loss: 0.6907 - acc: 0.5164\n",
      "Epoch 33/46\n",
      "50/50 [==============================] - 38s 770ms/step - loss: 0.6838 - acc: 0.5264\n",
      "Epoch 34/46\n",
      "50/50 [==============================] - 38s 752ms/step - loss: 0.6786 - acc: 0.5296\n",
      "Epoch 35/46\n",
      "50/50 [==============================] - 38s 760ms/step - loss: 0.6911 - acc: 0.5416\n",
      "Epoch 36/46\n",
      "50/50 [==============================] - 38s 758ms/step - loss: 0.6817 - acc: 0.5338\n",
      "Epoch 37/46\n",
      "50/50 [==============================] - 44s 888ms/step - loss: 0.6755 - acc: 0.5516\n",
      "Epoch 38/46\n",
      "50/50 [==============================] - 37s 738ms/step - loss: 0.6793 - acc: 0.5282\n",
      "Epoch 39/46\n",
      "50/50 [==============================] - 37s 744ms/step - loss: 0.6741 - acc: 0.5429\n",
      "Epoch 40/46\n",
      "50/50 [==============================] - 37s 749ms/step - loss: 0.6778 - acc: 0.5409\n",
      "Epoch 41/46\n",
      "50/50 [==============================] - 37s 749ms/step - loss: 0.6694 - acc: 0.5478\n",
      "Epoch 42/46\n",
      "50/50 [==============================] - 37s 748ms/step - loss: 0.6664 - acc: 0.5502\n",
      "Epoch 43/46\n",
      "50/50 [==============================] - 37s 749ms/step - loss: 0.6655 - acc: 0.5451\n",
      "Epoch 44/46\n",
      "50/50 [==============================] - 37s 745ms/step - loss: 0.6639 - acc: 0.5480\n",
      "Epoch 45/46\n",
      "50/50 [==============================] - 55s 1s/step - loss: 0.6648 - acc: 0.5400\n",
      "Epoch 46/46\n",
      "50/50 [==============================] - 42s 837ms/step - loss: 0.6620 - acc: 0.5476\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c2932cec18>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training model\n",
    "siamese_net.fit_generator(generator(train_data.transpose((0,2,1))),steps_per_epoch = len(train_data)/10, epochs = 46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "50/50 [==============================] - 13s 257ms/step - loss: 0.3376 - acc: 0.8722\n",
      "Epoch 2/10\n",
      "50/50 [==============================] - 14s 271ms/step - loss: 0.3256 - acc: 0.8736\n",
      "Epoch 3/10\n",
      "50/50 [==============================] - 15s 292ms/step - loss: 0.3532 - acc: 0.8533\n",
      "Epoch 4/10\n",
      "50/50 [==============================] - 14s 290ms/step - loss: 0.3516 - acc: 0.8676\n",
      "Epoch 5/10\n",
      "50/50 [==============================] - 14s 288ms/step - loss: 0.3336 - acc: 0.8800\n",
      "Epoch 6/10\n",
      "50/50 [==============================] - 14s 288ms/step - loss: 0.3133 - acc: 0.8900\n",
      "Epoch 7/10\n",
      "50/50 [==============================] - 14s 288ms/step - loss: 0.3203 - acc: 0.8807\n",
      "Epoch 8/10\n",
      "50/50 [==============================] - 15s 291ms/step - loss: 0.3206 - acc: 0.8776\n",
      "Epoch 9/10\n",
      "50/50 [==============================] - 14s 289ms/step - loss: 0.3098 - acc: 0.8884\n",
      "Epoch 10/10\n",
      "50/50 [==============================] - 15s 292ms/step - loss: 0.3000 - acc: 0.8887\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c290934b70>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siamese_net.fit_generator(generator(train_data.transpose((0,2,1))),steps_per_epoch = len(train_data)/10, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss and Accuracy\n",
      "[0.45208344534039496, 0.7977777749300003]\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss and Accuracy\")\n",
    "print(siamese_net.evaluate_generator(generator1(test_data.transpose((0,2,1))),steps=20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
